# -*- coding: utf-8 -*-
"""Telkom Intern Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zKrhto0OM6V56agUULaT05reMobv4PNe

# **Time Analytics**

# Modules
"""

# Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

from scipy import stats
from datetime import datetime
from scipy.stats import zscore

from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_absolute_error, mean_squared_error

"""# Dataset"""

# Dataset overview
df = pd.read_excel('/content/Dataset Internship Telkom.xlsx')
df

"""**Columns Understanding**

* Source = Database data source
* Periode = Incoming case time
* CaseNumber = Case number (primary key)
* Product = Product
* Status = Case status
* Symptom = Case detail
* Channel = Case channel
* Type = type of the case or symptom
* ClosedDate = Case closed time
* ResolvedTime_Second = Resolved case's time (in seconds)
"""

# Check the first and recent data time in the dataset
df['Periode'].min(), df['Periode'].max()

# Check the data types
df.info()

# Check the total missing value for each column
df.isnull().sum()

"""# Data Cleaning"""

# Check the 'primary key' duplicated data
df['CaseNumber'].duplicated().any()

# Drop irrelevant data
total_row_before = len(df)
zero_resolved_time_channels = df[df['ResolvedTime_Second'] == 0]['Channel']
df = df[df['ResolvedTime_Second'] != 0]
total_row_after = len(df)

total_removed_row = total_row_before - total_row_after

print("Total:", total_removed_row)
print(zero_resolved_time_channels)

# Data testing only?
df = df.copy()
df.drop(df[df['Symptom'] == 'Test'].index, inplace=True)

# Check whether the closed date is before the period
periode_closeddate = df[['Periode','ClosedDate']]
# periode_closeddate.head()

closeddate_before_periode = (periode_closeddate['ClosedDate'] < periode_closeddate['Periode']).any()
closeddate_before_periode

"""# Data Transformation

## Symptom & Type
"""

# Type column's unique values
df['Type'].unique()

# Symptom's unique values
df['Symptom'].unique()

# Fix the character
df['Symptom'] = df['Symptom'].str.replace('\xa0', ' ').str.lstrip()

"""There's inconsistency when inputting the description between Symptom with the Type. For instance, "Info..." in the Symptom column should be categorized as "Infomation". Moreover, "Users not respond" should be in "Other", not in "Incident" especially "Complaint"

"""

# Symptom - Type for 'Information'
df.loc[df['Symptom'].str.contains('Info', case=False, na=False), 'Type'] = 'Information'

# Make all words in the lowercase
df['Symptom'] = df['Symptom'].str.lower()

# Symptom - Type for 'User no respond, etc'
words = ['User tidak merespon', 'user no respon', 'user not respon', 'user tidak merespon']
def replace_type(row):
    for word in words:
        if word.lower() in str(row['Symptom']).lower():
            return 'Other'
    return row['Type']

df['Type'] = df.apply(replace_type, axis=1)

"""## Channel"""

# Channel column's unique values
df['Channel'].unique()

# Transform similar Channel's unique values based on the lettercase
df['Channel'] = df['Channel'].str.replace('whatsapp', 'WhatsApp', case=False, regex=True)

"""## Status"""

# Status column's unique values
df['Status'].unique()

# Transform similar Status column's unique values based on the lettercase
df['Status'] = df['Status'].str.replace('CLOSED', 'Closed', case=False, regex=True)

# Transform similar Status column's unique values
df['Status'] = df['Status'].replace({'On Hold': 'In Progress', 'New':'Open', 'Pending': 'In Progress', "Resolved":"Closed"})

"""## Source"""

# Source column's unique values
df['Source'].unique()

"""## Product"""

# Product column's unique values
df['Product'].unique()

# Create IndibizPAY and SooltanPay as one product
df['Product'] = df['Product'].str.replace('IndibizPay', 'SooltanPay', case=False, regex=True)

# Name the new product
df['Product'] = df['Product'].str.replace('SooltanPay', 'S&I Pay', case=False, regex=True)

"""## ClosedDate & Status"""

# Anomaly data count
null_date_closed_count = len(df.loc[(df['Status'] == 'Closed') & df['ClosedDate'].isna()])
print('Null closed date with closed/resolved status:', null_date_closed_count)

# Search and count for the Channel
whose_channel = df[(df['Status'] == 'Closed') & df['ClosedDate'].isna()]['Channel']
whose_channel = pd.DataFrame(whose_channel)
whose_channel['Channel'].value_counts()

# If there is a close date, make the status Closed. Conversely, if there is no close data but the status is closed, make it In Progress
for index, row in df.iterrows():
    if not pd.isnull(row['ClosedDate']):
        df.at[index, 'Status'] = 'Closed'
    elif pd.isnull(row['ClosedDate']) and row['Status'] == 'Closed':
        df.at[index, 'Status'] = 'In Progress'

"""## Resolved Time"""

# Just check the type
df['ResolvedTime_Second'].dtypes

"""## Category (New Column)"""

# Categorize
def categorize_type(type_value):
    if type_value in ['Incident', 'Complaint']:
        return 0  # 'Bad'
    else:
        return 1  # 'Normal/Good'
df['Category'] = df['Type'].apply(categorize_type)

"""# Handle Missing Values

The dataset has 3 columns that have missing values, let's have it done and find some insights and useful information from it.
"""

# Total rows who have missing value
rows_with_missing = df[df.isnull().any(axis=1)]
rows_missing_total = rows_with_missing.shape[0]
rows_missing_total

"""Create a function to match and group the columns"""

def count_missing_values_by_column(df_copy, group_column, columns_to_count):
    grouped = df_copy.groupby(group_column).agg({col: lambda x: x.isnull().sum() for col in columns_to_count}).reset_index()
    renamed_columns = [f'missing_{col}' for col in columns_to_count]
    grouped.columns = [group_column] + renamed_columns

    return grouped

# Total missing value by Channel's values
missing_values_by_channel = count_missing_values_by_column(df, 'Channel', ['Symptom', 'ClosedDate', 'ResolvedTime_Second'])
missing_values_by_channel

# Total missing value by the Source's values
missing_values_by_source = count_missing_values_by_column(df, 'Source', ['Symptom', 'ClosedDate', 'ResolvedTime_Second'])
missing_values_by_source

"""## "Symptom" Column"""

symptom_missing = df.loc[df['Symptom'].isnull(), ['Symptom', 'Type', 'Product']]
symptom_missing.head()

symptom_missing['Product'].unique()

symptom_missing['Type'].unique()

incident_symptom_df = df.loc[df['Type'] == 'Incident', 'Symptom']
incident_symptom_df

SMOOA_symptom_df = df.loc[df['Product'] == 'SMOOA', 'Symptom']
SMOOA_symptom_df.head()

word1 = ['tambah']
word2 = ['renewal']
total_count1 = df['Symptom'].str.lower().str.count('|'.join(word1)).sum()
total_count2 = df['Symptom'].str.lower().str.count('|'.join(word2)).sum()
print(f'Total count of target words: {total_count1}')
print(f'Total count of target words: {total_count2}')

df['Symptom'].fillna('Gagal Renewal', inplace=True)
df['Symptom'].isnull().sum()

"""## "ClosedDate" Column"""

# Count the null values
df['ClosedDate'].isnull().sum()

# Check the null values by product
closeddate_missing = df.loc[df['ClosedDate'].isnull(), ['Product']]
# closeddate_missing.head()

# Create a function that calculates the average of the total resolved time second by product
def calculate_average_total_date(df, product_names):
    result = {}

    for product_name in product_names:
        product_df = df.loc[df['Product'] == product_name].copy()
        product_df['TotalDate'] = (product_df['ClosedDate'] - product_df['Periode']).dt.days
        average_total_date = product_df['TotalDate'].mean()

        mask = (df['Product'] == product_name) & df['ClosedDate'].isna()
        df.loc[mask, 'ClosedDate'] = df.loc[mask, 'Periode'] + pd.to_timedelta(average_total_date, unit='D')

        result[product_name] = average_total_date
    return result

product_names = ['Pijar Mahir', 'S&I Pay', 'MyIndihome', 'SMOOA', 'T-Money']
average_total_dates = calculate_average_total_date(df, product_names)

for product_name, average_total_date in average_total_dates.items():
  print(f'Average total day(s) for {product_name} is: {average_total_date}')

# Fix the ClosedDate data type
df['ClosedDate'].dtypes

df['ClosedDate'] = pd.to_datetime(df['ClosedDate'])

# Check the null value
df['ClosedDate'].isnull().sum()

"""## "ResolvedTime_Second" Column"""

# Count the null values
df['ResolvedTime_Second'].isnull().sum()

df.loc[:, 'ResolvedTime_Second'] = df['ClosedDate'] - df['Periode']
df['ResolvedTime_Second']

# Fix the data type
df['ResolvedTime_Second'].dtype

df['ResolvedTime_Second'] = pd.to_timedelta(df['ResolvedTime_Second'])

# Brings ResolvedTime_Second's value to be in seconds
df['ResolvedTime_Second'] = df['ResolvedTime_Second'].dt.total_seconds()

# Create a new sorted df (by date/period)
ori_df = df.sort_values(by='Periode').reset_index(drop=True)

# Fix the data format/type again
ori_df['Periode'] = pd.to_datetime(ori_df['Periode'])
ori_df['ClosedDate'] = ori_df['ClosedDate'].dt.strftime('%Y-%m-%d %H:%M:%S')
ori_df['ClosedDate'] = pd.to_datetime(ori_df['ClosedDate'])

# Create new columns
ori_df['Year'] = ori_df['Periode'].dt.year
ori_df['Month'] = ori_df['Periode'].dt.month

ori_df

# Check the data
ori_df.info()

ori_df.describe()

"""# Visualization

## Univariate

### Channel and Product Distribution
"""

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.countplot(x='Channel', data=ori_df, palette="mako", ax=axes[0])
axes[0].set_title('Channel Distribution', fontsize = 14)
axes[0].set_xlabel('Channel', fontsize = 12)
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)
axes[0].grid(True, linestyle='--', alpha=0.2)

sns.countplot(x='Product', data=ori_df, palette="muted", ax=axes[1])
axes[1].set_title('Product Distribution', fontsize=14)
axes[1].set_xlabel('Product', fontsize=12)
axes[1].set_ylabel('Count', fontsize=12)
axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)
axes[1].grid(True, linestyle='--', alpha=0.2)

plt.show()

"""### Type and Source Distribution"""

source_counts = ori_df['Source'].value_counts(normalize=True, dropna=False)
type_counts = ori_df['Type'].value_counts(normalize=True, dropna=False)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.countplot(x='Type', data=ori_df, palette="colorblind", ax=axes[0])
axes[0].set_title('Type Distribution', fontsize=14)
axes[0].set_xlabel('Type', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)
axes[0].grid(True, linestyle='--', alpha=0.2)

source_colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightsalmon']
source_labels = source_counts.index

axes[1].pie(source_counts, labels=source_labels, colors=source_colors, autopct='%1.1f%%', shadow=True, startangle=140)
axes[1].set_title('Source Distribution', fontsize=16)
axes[1].axis('equal')

plt.show()
type_counts

"""*Type distribution is not reflects the actual condition

### Uncorrect Status Distribution
"""

uncorrect_status_counts = whose_channel['Channel'].value_counts(normalize=True, dropna=False)

plt.figure(figsize=(7, 5))
sns.countplot(x='Channel', data=whose_channel, palette="Set3")
plt.title('Uncorrect Status', fontsize=14)
plt.xlabel('Channel', fontsize=12)
plt.ylabel('Count', fontsize=12)

plt.show()
uncorrect_status_counts

"""## Multivariate

### Weekly Data
"""

weekly_data = ori_df.groupby(ori_df['Periode'].dt.to_period('W')).size().reset_index(name='Total_Data')
weekly_data.head()

weekly_data['Periode'] = weekly_data['Periode'].dt.to_timestamp()
weekly_data.set_index('Periode', inplace=True)

plt.figure(figsize=(16, 4))
sns.lineplot(data=weekly_data, x=weekly_data.index, y=weekly_data.Total_Data)
plt.grid(True, linestyle='--', alpha=0.2)
plt.show()

"""### Monthly Data"""

monthly_data = ori_df.groupby(ori_df['Periode'].dt.to_period('M')).size().reset_index(name='Total_Data')
monthly_data

monthly_data['Periode'] = monthly_data['Periode'].dt.to_timestamp()
monthly_data.set_index('Periode', inplace=True)

plt.figure(figsize=(16, 4))
sns.lineplot(data=monthly_data, x=monthly_data.index, y=monthly_data.Total_Data)
plt.grid(True, linestyle='--', alpha=0.2)
plt.show()

plt.figure(figsize=(8, 5))
plt.bar(monthly_data.index, monthly_data['Total_Data'], width=20, align='center', color='darkblue')
plt.xlabel('Periode')
plt.ylabel('Total Data')
plt.title('Monthly Data Distribution')
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(True, linestyle='--', alpha=0.2)
plt.show()

"""### Monthly Product Data Count"""

product_counts = ori_df.groupby(['Year', 'Month', 'Product']).size().reset_index(name='Count')

product_counts.head()

"""#### Barplot"""

plt.figure(figsize=(13, 8))
sns.barplot(data=product_counts, x='Month', y='Count', hue='Product', palette='colorblind')
plt.title('Total of Each Products Data Every Month', fontsize=14)
plt.xlabel('Month (in 2023)', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.legend(title='Product', title_fontsize='12')
plt.grid(True, linestyle='--', alpha=0.2)
plt.show()

"""#### Lineplot"""

plt.figure(figsize=(12, 6))

for product in product_counts['Product'].unique():
    product_data = product_counts[product_counts['Product'] == product]
    sns.lineplot(data=product_data, x='Month', y='Count', label=product, linewidth=2)

plt.title('Product Count Over Time', fontsize=14)
plt.xlabel('Month (in 2023)', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.legend(title='Product', title_fontsize='12', loc='upper left')
plt.grid(True, linestyle='--', alpha=0.2)

plt.show()

"""### Category vs 'Everybody'"""

# Create functions for the visualization
def plot_stack1(data, colors, x_label=' ', y_label=' ', title=' '):
    percentage_data = (data.T * 100.0 / data.T.sum()).T
    ax = percentage_data.plot(kind='bar', width=0.3, stacked=True, rot=0, figsize=(10, 5), color=colors)

    plt.ylabel(f'{y_label}\n', horizontalalignment="center", fontstyle="normal", fontfamily="sans-serif")
    plt.xlabel(f'{x_label}\n', horizontalalignment="center", fontstyle="normal", fontfamily="sans-serif")
    plt.title(f'{title}\n', horizontalalignment="center", fontstyle="normal", fontfamily="sans-serif")
    plt.legend(loc='upper right')
    plt.xticks(rotation=0, horizontalalignment="center")
    plt.yticks(rotation=0, horizontalalignment="right")
    plt.grid(True, linestyle='--', alpha=0.2)

    ax.yaxis.set_major_formatter(mtick.PercentFormatter())

    for p in ax.patches:
        width, height = p.get_width(), p.get_height()
        x, y = p.get_xy()
        percentage = height
        if percentage != 0:
            label_text = '{:.1f}%'.format(percentage)
            if percentage == 100:
                label_text = '100%'
            ax.text(x + width / 2,
                    y + height / 2,
                    label_text,
                    horizontalalignment='center',
                    verticalalignment='center')

    ax.autoscale(enable=False, axis='both', tight=False)
    plt.show()

def plot_stack2(data, x_label=' ', y_label=' ', title=' ', colors=None):
    ax = data.plot(kind='bar', width=0.3, stacked=True, rot=0, figsize=(10, 5), color=colors)

    plt.ylabel(f'{y_label}\n', horizontalalignment="center", fontstyle="normal", fontfamily="sans-serif")
    plt.xlabel(f'{x_label}\n', horizontalalignment="center", fontstyle="normal", fontfamily="sans-serif")
    plt.title(f'{title}\n', horizontalalignment="center", fontstyle="normal", fontfamily="sans-serif")
    plt.legend(loc='upper right')
    plt.xticks(rotation=0, horizontalalignment="center")
    plt.yticks(rotation=0, horizontalalignment="right")
    plt.grid(True, linestyle='--', alpha=0.2)

    for p in ax.patches:
        width, height = p.get_width(), p.get_height()
        x, y = p.get_xy()
        label_text = f'{height:.2f}'
        ax.text(x + width / 2,
                y + height / 2,
                label_text,
                horizontalalignment='center',
                verticalalignment='center')

    ax.autoscale(enable=False, axis='both', tight=False)
    plt.show()

"""#### Category vs Product"""

category_product = ori_df.groupby(['Product','Category']).size().unstack()
category_product.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(category_product, colors, y_label='Proportion of Customer', x_label='Product', title='Category vs Product')

"""#### Category vs Status"""

category_status = ori_df.groupby(['Status','Category']).size().unstack()
category_status.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(category_status, colors, y_label='Proportion of Customer', x_label='Status', title='Category vs Status')

"""#### Category vs Channel"""

category_channel = ori_df.groupby(['Channel','Category']).size().unstack()
category_channel.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(category_channel, colors, y_label='Proportion of Customer', x_label='Channel', title='Category vs Channel')

"""#### Category vs Month"""

category_month = ori_df.groupby(['Month','Category']).size().unstack()
category_month.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(category_month, colors, y_label='Proportion of Customer', x_label='Month', title='Category vs Month')

"""#### Category vs Product vs Month"""

category_month_df = ori_df.pivot_table(ori_df, index=['Year', 'Month', 'Product'], columns='Category', aggfunc='size', fill_value=0)
category_month_df.columns = ['Bad', 'Good']
category_month_df.reset_index(inplace=True)

# Count
category_month_df.head()

monthly_totals = category_month_df.groupby(['Month', 'Product'])[['Bad', 'Good']].sum().reset_index()
monthly_counts = category_month_df.groupby(['Month', 'Product'])['Bad'].count().reset_index()

plt.figure(figsize=(10, 8))

plt.subplot(2, 1, 1)
for product in monthly_totals['Product'].unique():
    product_data = monthly_totals[monthly_totals['Product'] == product]
    plt.plot(product_data['Month'], product_data['Bad'], label=product)
plt.title('Total Bad Category for Each Product Over Months')
plt.xlabel('Month')
plt.ylabel('Total Bad')
plt.grid(True, linestyle='--', alpha=0.2)
plt.legend()

plt.subplot(2, 1, 2)
for product in monthly_totals['Product'].unique():
    product_data = monthly_totals[monthly_totals['Product'] == product]
    plt.plot(product_data['Month'], product_data['Good'], label=product)
plt.title('Total Good Category for Each Product Over Months')
plt.xlabel('Month')
plt.ylabel('Total Good')
plt.grid(True, linestyle='--', alpha=0.2)
plt.legend()

plt.tight_layout()
plt.show()

# # Count
# df_category_product = category_month_df.groupby(['Month', 'Product'])[['Bad', 'Good']].sum().reset_index()
# df_category_product = pd.DataFrame(df_category_product)
# df_category_product.head()

"""# Human Resource Planning & Improve Customer Experience Analysis

## Additional Cleaning

karena berkaitan dengan resolved time yang tidak subjektif, maka dibuat New DF, Whatsapp di bawah 81 detik, berdasarkan Live Chat, karena new df, export semua column
"""

# Delete WhatsApp value in the Channel column that has ResolvedTime_Second less than 81 seconds
cleaned_df = ori_df[~((ori_df['Channel'] == 'WhatsApp') & ((ori_df['Type'] == 'Incident') | (ori_df['Type'] == 'Complaint')) & (ori_df['ResolvedTime_Second'] < 81))]

cleaned_df.tail()

"""## Time To Resolution (TTR)

### ResolvedTime_Second Mean by Product and Category
"""

# Average total ResolvedTime_Second by Categoty for each product
average_product_category = cleaned_df.groupby(['Product', 'Category'])['ResolvedTime_Second'].mean().reset_index()
average_product_category = average_product_category.rename(columns={'ResolvedTime_Second': 'Mean_ResolvedTime_Second'})
average_product_category

"""### ResolvedTime_Second Mean by the Top 5 Fastest and Slowest Symptoms"""

# Top 5 fastest and slowest Symptoms (General)
average_top_5_symptoms = cleaned_df.groupby(['Product', 'Symptom'])['ResolvedTime_Second'].mean().reset_index()
average_top_5_symptoms = average_top_5_symptoms.rename(columns={'ResolvedTime_Second': 'Mean_ResolvedTime_Second'})
average_top_5_symptoms

# Identify the symptom category
average_top_5_symptoms['Count'] = 0

for symptom in average_top_5_symptoms['Symptom']:
    count = (cleaned_df['Symptom'] == symptom).sum()
    average_top_5_symptoms.loc[average_top_5_symptoms['Symptom'] == symptom, 'Count'] = count

category_counts = cleaned_df.groupby(['Symptom', 'Category'])['Category'].count().unstack(fill_value=0)
average_top_5_symptoms = average_top_5_symptoms.merge(category_counts, on='Symptom', how='left')
average_top_5_symptoms = average_top_5_symptoms.rename(columns={0: 'Total_Category_0', 1: 'Total_Category_1'})

average_top_5_symptoms

# Top 5 Symptom (General)
sorted_top_5_symptom = average_top_5_symptoms.sort_values(by='Mean_ResolvedTime_Second')

top_5_fastest_symptoms = sorted_top_5_symptom.head(5)
top_5_slowest_symptoms = sorted_top_5_symptom.tail(5)

# Top 5 fastest
top_5_fastest_symptoms

# Top 5 slowest
top_5_slowest_symptoms

# Check the duplicate symptoms in the same product
duplicate_symptoms = sorted_top_5_symptom[sorted_top_5_symptom.duplicated(subset=['Symptom'], keep=False)]
print("Duplicate Symptoms:")
print(duplicate_symptoms)

"""#### Pijar Mahir"""

# Top 5 Fastest and Slowest Symptoms (Pijar Mahir)
pijar_mahir_display = 'Pijar Mahir'
pijar_mahir_symptom = sorted_top_5_symptom[sorted_top_5_symptom['Product'] == pijar_mahir_display]

# Symptoms that have a little number of case
pijar_mahir_symptom_rare = pijar_mahir_symptom[pijar_mahir_symptom['Count'] <= 8]

pijar_mahir_symptom.drop(pijar_mahir_symptom_rare.index, inplace=True)

top_5_fastest_pijar_mahir = pijar_mahir_symptom.head(5)
top_5_slowest_pijar_mahir = pijar_mahir_symptom.tail(5)

top_5_fastest_pijar_mahir

top_5_slowest_pijar_mahir

"""#### S&I Pay"""

# Top 5 fastest and slowest Symptoms (S&I Pay)
s_and_i_pay_display = 'S&I Pay'
s_and_i_pay_symptom = sorted_top_5_symptom[sorted_top_5_symptom['Product'] == s_and_i_pay_display]
top_5_fastest_s_and_i_pay = s_and_i_pay_symptom.head(5)
top_5_slowest_s_and_i_pay = s_and_i_pay_symptom.tail(5)

# Symptoms that have a small number of case
s_and_i_pay_symptom_rare = s_and_i_pay_symptom[s_and_i_pay_symptom['Count'] <= 8]

s_and_i_pay_symptom.drop(s_and_i_pay_symptom_rare.index, inplace=True)

top_5_fastest_s_and_i_pay = s_and_i_pay_symptom.head(5)
top_5_slowest_s_and_i_pay = s_and_i_pay_symptom.tail(5)

top_5_fastest_s_and_i_pay

top_5_slowest_s_and_i_pay

"""#### T-Money"""

# Top 5 fastest and slowest Symptoms (T-Money)
t_money_display = 'T-Money'
t_money_symptom = sorted_top_5_symptom[sorted_top_5_symptom['Product'] == t_money_display]

# Symptoms that have a small number of case
t_money_symptom_rare = t_money_symptom[t_money_symptom['Count'] <= 8]

t_money_symptom.drop(t_money_symptom_rare.index, inplace=True)

top_5_fastest_t_money = t_money_symptom.head(5)
top_5_slowest_t_money = t_money_symptom.tail(5)

top_5_fastest_t_money

top_5_slowest_t_money

"""#### SMOOA"""

# Top 5 fastest and slowest Symptoms (SMOOA)
SMOOA_display = 'SMOOA'
SMOOA_symptom = sorted_top_5_symptom[sorted_top_5_symptom['Product'] == SMOOA_display]

# Symptoms that have a small number of case
SMOOA_symptom_rare = SMOOA_symptom[SMOOA_symptom['Count'] <= 8]

SMOOA_symptom.drop(SMOOA_symptom_rare.index, inplace=True)

top_5_fastest_SMOOA = SMOOA_symptom.head(5)
top_5_slowest_SMOOA = SMOOA_symptom.tail(5)

top_5_fastest_SMOOA

top_5_slowest_SMOOA

"""#### MyIndihome"""

# Top 5 fastest and slowest Symptoms (MyIndihome)
MyIndihome_display = 'MyIndihome'
MyIndihome_symptom = sorted_top_5_symptom[sorted_top_5_symptom['Product'] == MyIndihome_display]

# Symptoms that have a small number of case
MyIndihome_symptom_rare = MyIndihome_symptom[MyIndihome_symptom['Count'] <= 8]

MyIndihome_symptom.drop(MyIndihome_symptom_rare.index, inplace=True)

top_5_fastest_MyIndihome = MyIndihome_symptom.head(5)
top_5_slowest_MyIndihome = MyIndihome_symptom.tail(5)

top_5_fastest_MyIndihome

top_5_slowest_MyIndihome

"""### Category ResolvedTime_Second Mean"""

# Category by ResolvedTime_Second
mean_time_per_category = cleaned_df.groupby('Category')['ResolvedTime_Second'].mean()
mean_time_per_category = pd.DataFrame(mean_time_per_category)
mean_time_per_category

"""## Frequency and Severity"""

# Create a new df name
busy_days = cleaned_df.copy()

busy_days

product_means = busy_days.groupby('Product')['ResolvedTime_Second'].mean().reset_index()
product_means.rename(columns={'ResolvedTime_Second': 'ProductMean'}, inplace=True)

busy_days = busy_days.merge(product_means, on='Product', how='left')

# Make the diff
busy_days['TimeDifference'] = busy_days['ResolvedTime_Second'] - busy_days['ProductMean']

# Delete the outlier which 3x than the std
std_multiplier = 3
outlier_threshold = std_multiplier * busy_days['TimeDifference'].std()

data_cleaned = busy_days[abs(busy_days['TimeDifference']) <= outlier_threshold]

# Drop and assign to busy_days dataframe
columns_to_drop = ['ProductMean', 'TimeDifference']
busy_days = data_cleaned.drop(columns=columns_to_drop)

# Create days name
busy_days['DayOfWeek'] = cleaned_df['Periode'].dt.day_name()
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

"""### The Most Symptom Value

"""

top_symptoms = busy_days['Symptom'].value_counts()
top_10_symptoms = top_symptoms.head(10)

print(top_10_symptoms)

top_10_symptoms = pd.DataFrame(top_10_symptoms)
top_10_symptoms

"""### Day Analysis"""

busy_days['Day'] = busy_days['Periode'].dt.strftime('%A')

"""#### Resolved time mean per day"""

# # ResolvedTime_Second mean each day
# RTS_day = busy_days.groupby('DayOfWeek').agg({'Periode': 'count', 'ResolvedTime_Second': 'mean'})
# RTS_day = RTS_day.reindex(day_order)

busy_days['DayOfWeek'] = busy_days['Periode'].dt.dayofweek
rts_day = busy_days.groupby('DayOfWeek').agg({'ResolvedTime_Second': 'mean', 'Periode': 'count'})
rts_day.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
rts_day.columns = ['ResolvedTime_Second Mean', 'Count']
rts_day = rts_day.reindex(day_order)

print(rts_day)

"""#### Estimated unfinished ticket/case each day"""

# Total Period and ClosedDate
period_counts = busy_days['Periode'].dt.day_name().value_counts().rename('PeriodCount')
closed_date_counts = busy_days['ClosedDate'].dt.day_name().value_counts().rename('ClosedDateCount')

# Unfinished
unfinished_day = pd.concat([period_counts, closed_date_counts], axis=1)
unfinished_day = unfinished_day.reindex(day_order, fill_value=0)
unfinished_day['Diff'] = unfinished_day['PeriodCount'] - unfinished_day['ClosedDateCount']

print(unfinished_day)

# Total of Data
count_sum_period = unfinished_day['PeriodCount'].sum()
count_sum_closed = unfinished_day['ClosedDateCount'].sum()

print('Total of Period count:', count_sum_period)
print('Total of Closed Date count:',count_sum_closed)

"""#### Estimated number of actual cases worked on each day"""

# Estimation
estimated_actual_case = unfinished_day.drop(columns=['Diff'])
remainder = 0

for index, row in estimated_actual_case.iterrows():
    period_count = row['PeriodCount']
    closed_date_count = row['ClosedDateCount']
    period_count += remainder
    diff = period_count - closed_date_count
    remainder = diff
    estimated_actual_case.at[index, 'ClosedDateCount'] = 0
    estimated_actual_case.at[index, 'PeriodCount'] = period_count

estimated_actual_case = estimated_actual_case.drop(columns=['ClosedDateCount'])
estimated_actual_case = estimated_actual_case.rename(columns={'PeriodCount': 'Estimated Total Case'})

print(estimated_actual_case)

"""#### Each product on each day"""

# Product Case by Day
products = ['MyIndihome', 'Pijar Mahir', 'S&I Pay', 'SMOOA', 'T-Money']
product_counts = busy_days[busy_days['Product'].isin(products)].groupby(['Day', 'Product'])['Product'].count().unstack(fill_value=0)
product_counts = product_counts.reindex(day_order)

print(product_counts)

"""#### Estimated Product and Category on each day"""

# Product and Category by day
counted_category = busy_days['Category'].unique()
product_category_counts = busy_days[busy_days['Product'].isin(products) & df['Category'].isin(counted_category)].groupby(['Day', 'Product', 'Category'])['Product'].count().unstack(fill_value=0)

print(product_category_counts)

"""#### Unfinished case estimation for Product and Category on each day"""

# Unfinished Case by Category and Product
busy_days['TimePeriod'] = busy_days['ClosedDate'] - busy_days['Periode']
busy_days['DayOfWeek_Period'] = busy_days['Periode'].dt.day_name()

# Filter: TimePeriod > 1 day
time_period_more_than_one = busy_days[busy_days['TimePeriod'] > pd.Timedelta(days=1)]

time_period_more_than_one = time_period_more_than_one.groupby(['DayOfWeek_Period', 'Product', 'Category']).size().reset_index(name='Count')

unfinished_category_product = time_period_more_than_one.pivot_table(index=['DayOfWeek_Period', 'Product'], columns='Category', values='Count', fill_value=0)
days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
unfinished_category_product = unfinished_category_product.reindex(index=pd.MultiIndex.from_product([days_order, unfinished_category_product.index.levels[1]]))

print(unfinished_category_product)

"""#### ResolvedTime_Second mean for unfinished case by Product and Category on each day"""

# Unfinished Case by Category Mean and Product
busy_days['TimePeriod'] = (busy_days['ClosedDate'] - busy_days['Periode']).dt.total_seconds()

more_than_1_day = busy_days[busy_days['TimePeriod'] >= 86400]

unfinished_category_mean_product = more_than_1_day.groupby(['DayOfWeek_Period', 'Product', 'Category'])['TimePeriod'].mean().unstack()
days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

unfinished_category_mean_product = unfinished_category_mean_product.loc[days_order]
unfinished_category_mean_product = unfinished_category_mean_product.fillna(0)

print(unfinished_category_mean_product)

"""### All Dates by Product and Category"""

# All dates for total case
day_counts = busy_days.resample('D', on='Periode').agg({'Category': ['count', lambda x: x.eq(0).sum(), lambda x: x.eq(1).sum()]})

day_counts.columns = ['Total_Data', 'Category_0', 'Category_1']
day_counts['Day'] = day_counts.index.strftime('%A')
day_counts.reset_index(inplace=True)

print(day_counts)

"""### Hour Mode for Each Day"""

busy_days['Hari'] = busy_days['Periode'].dt.day_name()
busy_days['JamMasuk'] = busy_days['Periode'].dt.hour

# Hitung jumlah kasus masuk pada setiap jam untuk setiap hari
jumlah_kasus_per_jam_hari = busy_days.groupby(['Hari', 'JamMasuk'])['JamMasuk'].count()

# Temukan jam dengan jumlah kasus terbanyak untuk setiap hari
jam_terbanyak_hari = jumlah_kasus_per_jam_hari.groupby('Hari').idxmax()
jumlah_kasus_terbanyak_hari = jumlah_kasus_per_jam_hari.groupby('Hari').max()

print(jam_terbanyak_hari)
print(jumlah_kasus_terbanyak_hari)

"""# Visualization

## Time To Resolution (TTR)
"""

# ResolvedTime_Second Mean by Product and Category
palette = {0: 'red', 1: 'green'}

plt.figure(figsize=(10, 6))
sns.barplot(x='Product', y='Mean_ResolvedTime_Second', hue='Category', data=average_product_category, palette=palette)
plt.title('ResolvedTime_Second Mean by Product and Category')
plt.xlabel('Product')
plt.ylabel('ResolvedTime_Second Mean')
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

# Plot for top 5 fastest and slowest symptoms (general)
colors = ['#FF5733', '#FFC300', '#C70039', '#900C3F', '#581845']

plt.figure(figsize=(12, 7))

plt.subplot(1, 2, 1)
plt.bar(top_5_slowest_symptoms['Symptom'], top_5_slowest_symptoms['Mean_ResolvedTime_Second'], color=colors)
plt.xlabel('Symptom')
plt.ylabel('ResolvedTime_Seconds (Mean)')
plt.title('Top 5 Slowest Mean_ResolvedTime_Second')
plt.xticks(rotation=45, ha="right")
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.subplot(1, 2, 2)
plt.bar(top_5_fastest_symptoms['Symptom'], top_5_fastest_symptoms['Mean_ResolvedTime_Second'], color=colors)
plt.xlabel('Symptom')
plt.ylabel('ResolvedTime_Seconds (Mean)')
plt.title('Top 5 Fastest Mean_ResolvedTime_Second')
plt.xticks(rotation=45, ha="right")
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Functions for visualization
def plot_top_symptoms(top_fastest_df, top_slowest_df):
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    axes[0].barh(top_fastest_df['Symptom'], top_fastest_df['Mean_ResolvedTime_Second'], color='skyblue')
    axes[0].set_xlabel('ResolvedTime_Seconds (Mean)')
    axes[0].set_title('Top 5 Fastest Symptoms')
    axes[0].grid(True)
    axes[0].invert_yaxis()
    axes[0].grid(axis='y', linestyle='--', alpha=0.2)

    axes[1].barh(top_slowest_df['Symptom'], top_slowest_df['Mean_ResolvedTime_Second'], color='coral')
    axes[1].set_xlabel('ResolvedTime_Seconds (Mean)')
    axes[1].set_title('Top 5 Slowest Symptoms')
    axes[1].grid(True)
    axes[1].invert_yaxis()
    axes[1].grid(axis='y', linestyle='--', alpha=0.2)


    plt.tight_layout()
    plt.show()

def plot_symptom_categories(top_fastest_df, top_slowest_df):
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    def plot_single_category(ax, df, title):
        # Category 0 (limegreen)
        bar1 = ax.barh(df['Symptom'], df['Total_Category_0'], color='coral', label='Total_Category_0')

        # Category 1 (coral)
        bar2 = ax.barh(df['Symptom'], df['Total_Category_1'], left=df['Total_Category_0'], color='limegreen', label='Total_Category_1')

        for b1, b2 in zip(bar1, bar2):
            x1 = b1.get_width()
            x2 = b2.get_width()
            total = x1 + x2
            percentage1 = x1 / total * 100
            percentage2 = x2 / total * 100

            if percentage1 == 100:
                ax.text(x1 / 2, b1.get_y() + b1.get_height() / 2, f'{percentage1:.0f}%', ha='center', va='center', color='white')
            elif percentage2 == 100:
                ax.text(x1 + x2 / 2, b2.get_y() + b2.get_height() / 2, f'{percentage2:.0f}%', ha='center', va='center', color='white')
            else:
                ax.text(x1 / 2, b1.get_y() + b1.get_height() / 2, f'{percentage1:.1f}%', ha='center', va='center')
                ax.text(x1 + x2 / 2, b2.get_y() + b2.get_height() / 2, f'{percentage2:.1f}%', ha='center', va='center')

        ax.set_xlabel('Count')
        ax.set_title(title)
        ax.grid(axis='y', linestyle='--', alpha=0.2)
        ax.legend()
        ax.invert_yaxis()
        ax.grid(True)

    plot_single_category(axes[0], top_fastest_df, 'Top 5 Fastest Symptoms - Category Counts')
    plot_single_category(axes[1], top_slowest_df, 'Top 5 Slowest Symptoms - Category Counts')

    plt.tight_layout()
    plt.show()

"""### Pijar Mahir"""

plot_top_symptoms(top_5_fastest_pijar_mahir, top_5_slowest_pijar_mahir)

plot_symptom_categories(top_5_fastest_pijar_mahir, top_5_slowest_pijar_mahir)

"""### S&I Pay"""

plot_top_symptoms(top_5_fastest_s_and_i_pay, top_5_slowest_s_and_i_pay)

plot_symptom_categories(top_5_fastest_s_and_i_pay, top_5_slowest_s_and_i_pay)

"""### T-Money"""

plot_top_symptoms(top_5_fastest_t_money, top_5_slowest_t_money)

plot_symptom_categories(top_5_fastest_t_money, top_5_slowest_t_money)

"""### SMOOA"""

plot_top_symptoms(top_5_fastest_SMOOA, top_5_slowest_SMOOA)

plot_symptom_categories(top_5_fastest_SMOOA, top_5_slowest_SMOOA)

"""### MyIndihome"""

plot_top_symptoms(top_5_fastest_MyIndihome, top_5_slowest_MyIndihome)

plot_symptom_categories(top_5_fastest_MyIndihome, top_5_slowest_MyIndihome)

"""### Category-ResolvedTime_Second Mean"""

mean_time_per_category.reset_index(inplace=True)
mean_time_per_category.rename(columns={'index': 'Category'}, inplace=True)

# Plot for ResolvedTime_Second by Category
width = 0.8
x = range(len(mean_time_per_category))

plt.figure(figsize=(8, 5))
plt.bar(x, mean_time_per_category['ResolvedTime_Second'], width=width, color=['lightcoral', 'lightblue'])
plt.xlabel('Category')
plt.ylabel('ResolvedTime_Second')
plt.title('ResolvedTime_Second by Category')
plt.xticks(x, mean_time_per_category['Category'])
plt.grid(axis='y', linestyle='--', alpha=0.2)

plt.show()

"""## Frequency and Severity **(In Progress)**

## Category Comparison Visualization (Cleaned)

### Category vs Product
"""

cleaned_category_product = busy_days.groupby(['Product','Category']).size().unstack()
cleaned_category_product.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(cleaned_category_product, colors, y_label='Proportion of Customer', x_label='Product', title='Category vs Product (Cleaned)')

"""### Category vs Status"""

cleaned_category_status = busy_days.groupby(['Status','Category']).size().unstack()
cleaned_category_status.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(cleaned_category_status, colors, y_label='Proportion of Customer', x_label='Status', title='Category vs Status (Cleaned)')

"""### Category vs Channel"""

cleaned_category_channel = busy_days.groupby(['Channel','Category']).size().unstack()
cleaned_category_channel.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(cleaned_category_channel, colors, y_label='Proportion of Customer', x_label='Channel', title='Category vs Channel (Cleaned)')

"""### Category vs Month"""

cleaned_category_month = busy_days.groupby(['Month','Category']).size().unstack()
cleaned_category_month.rename(columns={0:'Bad', 1:'Good'}, inplace=True)
colors  = ['#ff7f0e','#17becf']
plot_stack1(cleaned_category_month, colors, y_label='Proportion of Customer', x_label='Month', title='Category vs Month (Cleaned)')

"""# Forecasting"""

# Use original dataset (df) for forecasting
day_counts_original = df.resample('D', on='Periode').agg({'Category': ['count', lambda x: x.eq(0).sum(), lambda x: x.eq(1).sum()]})

day_counts_original.columns = ['Total_Data', 'Category_0', 'Category_1']
day_counts_original['Day'] = day_counts_original.index.strftime('%A')
day_counts_original.reset_index(inplace=True)

print(day_counts_original)

"""## SARIMAX

### Data Cleaning
"""

fc_data = day_counts_original.drop(['Category_0', 'Category_1', 'Day'], axis=1)
# fc_data = day_counts.drop(['Total_Data', 'Category_1', 'Day'], axis=1)
# fc_data = day_counts.drop(['Category_0', 'Total_Data', 'Day'], axis=1)

# Make the DATE's value as an index, lag, or t
fc_data.set_index('Periode', inplace=True)

fc_data

"""### Original Data Plot"""

plt.figure(figsize=(16, 4))
sns.lineplot(data=fc_data, x=fc_data.index,y=fc_data.Total_Data)
plt.show()

# Do the decomposition
decomposition = sm.tsa.seasonal_decompose(fc_data['Total_Data'], model='additive')

# Decomposition components
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

# Plot the components
plt.figure(figsize=(12, 8))
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='upper left')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='upper left')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

# Define the window
fc_data['rollMean'] = fc_data.Total_Data.rolling(window=7).mean()
fc_data['rollStd'] = fc_data.Total_Data.rolling(window=7).std()

# Plot the rollMean and rollStd
plt.figure(figsize = (16,4))
sns.lineplot(data=fc_data, x=fc_data.index, y=fc_data.Total_Data)
sns.lineplot(data=fc_data, x=fc_data.index, y=fc_data.rollMean)
sns.lineplot(data=fc_data, x=fc_data.index, y=fc_data.rollStd)

"""### Stationarity Test

Null Hypothesis = The data is non stationary
"""

adfTest = adfuller(fc_data['Total_Data'],autolag = "AIC",) # Use AIC for choosing the number of lags
adfTest

stats_adf_test = pd.Series(adfTest[0:4],index=['Test Statistic', 'p-value','#lags used','number of observations used'])
stats_adf_test

"""p-value is lower than 0.05, Null Hypothesis can be rejected"""

# Drop the unnecessary columns
fc_data = fc_data.drop(columns=['rollMean', 'rollStd'])

"""### Determine p d q order"""

# Plot the Autocorrelation Function (ACF)
plt.figure(figsize=(10, 4))
ax1 = plt.subplot(121)
plot_acf(fc_data['Total_Data'], ax=ax1)

# Plot the Partial Autocorrelation Function (PACF)
ax2 = plt.subplot(122)
plot_pacf(fc_data['Total_Data'], ax=ax2)

plt.tight_layout()
plt.show()

# Test the p d q order
fc_data.index = pd.to_datetime(fc_data.index)
fc_data = fc_data.asfreq('D')
ar_model = SARIMAX(fc_data['Total_Data'], order=(1,0,1)).fit()
ar_model.summary()

"""### Train and Test the Data"""

# Use 21 steps or 3 weeks data for test and train
print(fc_data.shape)
train = fc_data['Total_Data'].iloc[:-21]
test = fc_data['Total_Data'].iloc[-21:]
print(train.shape, test.shape)

print(test.index, train.index)

test_df = test.to_frame()
train_df = train.to_frame()

# SARIMAX
order = (1, 0, 1)
seasonal_order = (1, 0, 1, 7)  # Seasonal order (P, D, Q, m)
model = sm.tsa.SARIMAX(train, order=order, seasonal_order=seasonal_order)
results = model.fit()

start = len(train)
end = len(train) + len(test) - 1

pred_sarimax = results.predict(start=start, end=end, typ='levels')

# Convert predictions into appropriate indices
pred_sarimax.index = fc_data.index[start:end + 1]
# print(results.summary())

plt.figure(figsize=(7, 6))
pred_sarimax.plot(legend=True, label='Test', color='red')
fc_data['Total_Data'].plot(legend=True, label='Train', color='green')
plt.title('SARIMAX Train vs Test')

plt.tight_layout()
plt.show()

train_fc= fc_data['Total_Data']
train_fc

"""### Forecast Future Value"""

# Fit the SARIMAX model to the log-transformed data
sarimax_model = SARIMAX(train_fc, order=(1, 0, 1), seasonal_order=(1, 0, 1, 7))
fitted_model = sarimax_model.fit()

# Get the forecasts from the SARIMAX model in log-transformed units
forecast_data_sarimax = fitted_model.forecast(steps=21)

# Predictions
predictions_SARIMAX = fitted_model.get_forecast(steps=len(test))

# Confidence intervals for the predictions
pred_confidence = predictions_SARIMAX.conf_int()

# Extend the time index for predictions
future_index = pd.date_range(start=train_fc.index[-1], periods=len(test), freq='D')

# Visualize the original time series (log) and the predictions
plt.figure(figsize=(15, 6))
plt.plot(fc_data.index, fc_data['Total_Data'], label=' Log "Original"', color='green')
plt.plot(future_index, predictions_SARIMAX.predicted_mean, color='blue', label='Log Predictions')
plt.xlabel('Year')
plt.ylabel('Electric Production')
plt.title('SARIMAX Log Value Forecast')
plt.legend()
plt.show()

forecast_data_sarimax

"""## Exponential Smoothing

### Forecast Future Value
"""

model = ExponentialSmoothing(fc_data['Total_Data'], seasonal='add', seasonal_periods=7, trend='add')

es_fit = model.fit()
forecast_es = es_fit.forecast(steps=21)

"""## Comparison"""

# Statistical comparison
sarima_mae = mean_absolute_error(forecast_data_sarimax, pred_sarimax)
sarima_mse = mean_squared_error(forecast_data_sarimax, pred_sarimax)
sarima_rmse = np.sqrt(sarima_mse)
sarima_aic = fitted_model.aic

true_values = fc_data['Total_Data'].tail(21)
es_mae = mean_absolute_error(true_values, forecast_es)
es_mse = mean_squared_error(true_values, forecast_es)
es_rmse = np.sqrt(es_mse)
es_aic = es_fit.aic

print("SARIMA MAE: ", sarima_mae)
print("Exponential Smoothing MAE: ", es_mae)

print("SARIMA MSE: ", sarima_mse)
print("Exponential Smoothing MSE: ", es_mse)

print("SARIMA RMSE: ", sarima_rmse)
print("Exponential Smoothing RMSE: ", es_rmse)

print("SARIMA AIC: ", sarima_aic)
print("Exponential Smoothing AIC: ", es_aic)

"""## Forecasting Result

Based on the MAE, MSE, AIC, and RMSE scores, SARIMAX has a better result

### Exponential Smoothing Result
"""

# Show the Exponential Smoothing result
print(forecast_es)

"""### SARIMAX Result"""

# Fit the SARIMAX result into dataframe
forecast_data_sarimax = pd.DataFrame(forecast_data_sarimax)

forecast_data_sarimax['Periode'] = forecast_data_sarimax.index
forecast_data_sarimax = forecast_data_sarimax.reset_index(drop=True)
forecast_data_sarimax = forecast_data_sarimax.rename(columns={'predicted_mean': 'predicted_total_case'})

# Show the result
forecast_data_sarimax['predicted_total_case'] = forecast_data_sarimax['predicted_total_case'].round()
forecast_data_sarimax

# Total predicted case by day
forecast_data_sarimax['Day_Name'] = forecast_data_sarimax['Periode'].dt.strftime('%A')
total_cases_per_day = forecast_data_sarimax.groupby('Day_Name')['predicted_total_case'].sum().reset_index()
total_cases_per_day['Day_Name'] = pd.Categorical(total_cases_per_day['Day_Name'], categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=True)
total_cases_per_day = total_cases_per_day.sort_values('Day_Name')

total_cases_per_day

# In progress